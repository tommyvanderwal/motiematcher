import json
from pathlib import Path
import time

def pandas_vs_json_assessment():
    """Quick assessment of pandas vs current JSON approach"""
    
    print("üìä PANDAS VS JSON PERFORMANCE ASSESSMENT")
    print("=" * 50)
    
    # Test current JSON approach speed
    zaak_files = list(Path('bronmateriaal-onbewerkt/zaak').glob('*.json'))
    vote_files = list(Path('bronmateriaal-onbewerkt/stemming').glob('*.json'))
    
    print(f"üß™ TEST 1: JSON Loading Speed")
    
    start_time = time.time()
    total_moties = 0
    total_votes = 0
    
    # Load all motion files
    for file in zaak_files:
        data = json.load(open(file, encoding='utf-8'))
        total_moties += len(data)
    
    # Load 5 vote files  
    for file in vote_files[:5]:
        data = json.load(open(file, encoding='utf-8'))
        total_votes += len(data)
    
    json_load_time = time.time() - start_time
    
    print(f"   ‚è±Ô∏è JSON approach: {json_load_time:.3f}s")
    print(f"   üìä Loaded {total_moties} moties + {total_votes} votes")
    
    print(f"\nüéØ PANDAS ASSESSMENT:")
    print(f"   Current data size: 22.9 MB")
    print(f"   Records: ~10,000 total")
    print(f"   JSON speed: {json_load_time:.3f}s for partial load")
    
    print(f"\n‚úÖ RECOMMENDATION:")
    if json_load_time < 1.0:
        print(f"   üèÉ‚Äç‚ôÇÔ∏è KEEP JSON: Current approach is fast enough")
        print(f"   üìã Benefits: Simple, flexible, no dependencies")
        print(f"   ‚ö° Performance: {json_load_time:.3f}s loading is excellent")
        
        print(f"\nüîÑ CONSIDER PANDAS LATER FOR:")
        print(f"   üîó Cross-dataset joins (motie + stemming)")
        print(f"   üìà Complex aggregations (party voting patterns)")
        print(f"   üèóÔ∏è Data transformation for web API")
        print(f"   üìä Analytics and reporting features")
        
    else:
        print(f"   üêº CONSIDER PANDAS: Loading is slow")
        
    return json_load_time

def estimate_full_dataset_size():
    """Estimate full dataset size with all entity types"""
    
    print(f"\nüìè FULL DATASET SIZE ESTIMATION")
    print("=" * 40)
    
    current_size = 22.9  # MB
    
    estimates = {
        "Zaak (all types)": current_size * 2,  # Assume 2x more than just moties
        "Document entities": current_size * 1.5,  # Rich text content
        "Activiteit entities": current_size * 0.8,  # Activity logs
        "Complete dataset": current_size * 4.5  # Total estimate
    }
    
    for entity_type, size_mb in estimates.items():
        pandas_threshold = 100  # MB where pandas becomes useful
        recommendation = "JSON OK" if size_mb < pandas_threshold else "Consider Pandas"
        print(f"   {entity_type:20}: {size_mb:5.1f} MB - {recommendation}")
    
    print(f"\nüéØ CONCLUSION:")
    total_estimated = estimates["Complete dataset"]
    if total_estimated < 100:
        print(f"   ‚úÖ Stick with JSON approach")
        print(f"   üìä Estimated full dataset: {total_estimated:.1f} MB")
        print(f"   üöÄ JSON will remain fast and flexible")
    else:
        print(f"   üêº Pandas recommended for full dataset")
        print(f"   üìä Estimated {total_estimated:.1f} MB may benefit from DataFrame operations")

if __name__ == "__main__":
    load_time = pandas_vs_json_assessment()
    estimate_full_dataset_size()
    
    print(f"\nüí° FINAL VERDICT:")
    print(f"   üéØ Current phase: Continue with JSON")  
    print(f"   üîÑ Next phase: Evaluate pandas for data transformation")
    print(f"   üåê Web API phase: Consider pandas for complex queries")